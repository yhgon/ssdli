{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNNs and text\n",
    "\n",
    "Sequence information is everywhere and classifying or forecasting it has applications in many different areas.  One area where RNNs have proven to be useful in recent years is in machine understanding of natural language text - this is often called Natural Language Processing (NLP). \n",
    "\n",
    "Text data can be viewed as a sequence at multiple resolutions - for example, as sequences of paragraphs, lines, sentences, words or individual characters.  \n",
    "\n",
    "![](files/lucky.png)\n",
    "\n",
    "In the same way that feed-forward and convolutional neural networks can be applied to classification or regression tasks, there are a variety of different tasks that RNNs can be applied to:\n",
    "- We can use an RNN to regress a value based on sequential inputs - this is what we did in our binary addition example.\n",
    "- We can use an RNN to classify it's input - an NLP example of this approach is determining the positive or negative sentiment of tweets.  \n",
    "- We can also use RNNs to forecast future values of sequences given their historical values - an NLP example of this approach is forecasting the next word in a partial sentence.  This can be very useful in real-time text translation between languages.\n",
    "\n",
    "We can also do all of these applications with either a single vector or whole sequence vectors as both input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Character level text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replicate an RNN training example made famous by Andrej Karpathy in his entertaining and informative blog post [\"The unreasonable effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).  In that post Andrej showed how an RNN can be trained on a text corpus, one character at a time, to predict the next character in the sequence.  Once trained in this way the network can be provided a seed of characters to begin a sequence and then asked to recursively predict as many additional characters in the sequence as we like.  If training has been successful the generated characters will form words, sentences and even paragraphs with the appropriate vocabulary, grammar and structure learned from the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to generate not only sensible next character guesses but whole words and sentences with correct grammar requires the network to have a memory of tens or even hundreds of characters in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we are going to use the [torch-rnn](https://github.com/ngimel/torch-rnn) codebase implemented by Justin Johnson (Stanford) and then accelerated with cuDNN by NVIDIA's own Natalia Gimelshein atop the scientific computing framework [Torch](http://torch.ch/).  Torch has wide support for GPU accelerated machine learning algorithms. It is easy to use and efficient, thanks to an easy and fast scripting language, LuaJIT, and an underlying C/CUDA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- First we will import the required torch modules\n",
    "\n",
    "package.path = './torch-rnn/?.lua;./torch-rnn/?/?.lua;' .. package.path\n",
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'cunn'\n",
    "require 'cudnn'\n",
    "require 'util.DataLoader'\n",
    "require 'LanguageModel'\n",
    "\n",
    "utils = require 'util.utils'\n",
    "unpack = unpack or table.unpack\n",
    "\n",
    "cutorch.setDevice(1)          -- we will use the first GPU\n",
    "dtype = 'torch.CudaTensor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a text file containing 40000 lines from the works of William Shakespeare as our training dataset.  The raw data is in the file [tiny-shakespeare](torch-rnn/data/tiny-shakespeare.txt).  torch-rnn expects the text characters of the training data to be converted to integers and stored in an HDF5 binary file and the mapping from text characters to integers to be stored in an accompanying [JSON file](torch-rnn/data/shakespeare.json). The `DataLoader` module from torch-rnn can iteratively generated mini-batches of data for training from the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Set parameters for the training files and minibatch generation\n",
    "opt = {}       -- We will use the opt dictionary to store all parameters related to the RNN training\n",
    "opt.input_h5 = 'torch-rnn/data/shakespeare.h5'\n",
    "opt.input_json = 'torch-rnn/data/shakespeare.json'\n",
    "opt.batch_size = 50     \n",
    "opt.seq_length = 50\n",
    "\n",
    "-- Instantiate a data loader to generate minibatches from the HDF5 file\n",
    "loader = DataLoader(opt)\n",
    "vocab = utils.read_json(opt.input_json)\n",
    "\n",
    "-- Create a dictionary mapping text characters to integers\n",
    "idx_to_token = {}\n",
    "for k, v in pairs(vocab.idx_to_token) do\n",
    "  idx_to_token[tonumber(k)] = v\n",
    "end\n",
    "opt.idx_to_token = idx_to_token\n",
    "\n",
    "-- Sample the first training batch and target\n",
    "-- The batch is opt.batch_size sequences each of length seq_length\n",
    "-- The RNN will be trained one character at a time from each sequence\n",
    "-- The target is the next character in the sequence\n",
    "x, y = loader:nextBatch('train')\n",
    "print(x:size())\n",
    "print(y:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN layers are implemented in torch-rnn as a subclass of Torch [nn.Module](https://github.com/torch/nn/blob/master/doc/module.md#nn.Module).   An RNN layers transforms a sequence of input vectors of dimension `D` into a sequence of hidden state vectors of dimension `H`; it operates over sequences of length `T` and minibatches of size `N`, which can be different on each forward pass.\n",
    "\n",
    "First we will show how to instantiate an RNN layer in torch-rnn and feed-forward some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Initialize an RNN layer\n",
    "N, T, D, H = 3, 4, 5, 6\n",
    "rnn = nn.VanillaRNN(D, H)\n",
    "\n",
    "-- Initialize some random input data\n",
    "x  = torch.randn(N, T, D)\n",
    "\n",
    "-- Initialize a random hidden state from the last time step\n",
    "h0 = torch.randn(N, H)\n",
    "\n",
    "-- Feed-forward the random data  \n",
    "h = rnn:forward{h0, x}\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN layer instantiated in this way also has a `backward` method that takes as input the original input to `forward` as well as the gradients computed with respect to the output of `forward`.  This function provides the means to perform gradient backpropagation on the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated applications we can extend the RNN structure we used in our first example in many of the same ways we can extend a feed-forward or convolutional neural network.  For example, we can increase the number of neurons in the hidden layer and we can stack multiple hidden layers to create a deep RNN architecture.  We can define a stacked RNN network for natural language processing and randomly initialize it on the GPU in Torch in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_size = 128    -- Number of neurons in each hidden layer\n",
    "num_layers = 2    -- Number of hidden layers\n",
    "\n",
    "vocab_size = 256  -- How many unique text characters appear in the training data corpus\n",
    "wordvec_dim = 64  -- What size of dense vector should be used to represent each character\n",
    "-- NOTE: a dense real-valued vector is often used to represent individual characters or words\n",
    "-- in a text sequence rather than sparse \"one-hot\" vectors\n",
    "\n",
    "V, D, H = vocab_size, wordvec_dim, rnn_size\n",
    "\n",
    "net = nn.Sequential()          -- Initialize an empty network architecture\n",
    "net:add(nn.LookupTable(V, D))  -- Add a lookup table layer for character vectors\n",
    "\n",
    "-- Iteratively stack RNN layers on the network\n",
    "for i = 1, num_layers do\n",
    "    local prev_dim = H\n",
    "    if i == 1 then prev_dim = D end\n",
    "    local rnn\n",
    "    rnn = nn.VanillaRNN(prev_dim, H)\n",
    "    net:add(rnn)\n",
    "end\n",
    "\n",
    "-- The next 5 layers are used to have the output of the network\n",
    "-- be a fully-connected linear layer based upon the values of the top \n",
    "-- hidden layer across all time-steps rather than just the last time-step.\n",
    "-- Don't worry about the details, just understand the purpose.\n",
    "view1 = nn.View(1, 1, -1):setNumInputDims(3)\n",
    "view2 = nn.View(1, -1):setNumInputDims(2)\n",
    "net:add(view1)\n",
    "net:add(nn.Linear(H, V))\n",
    "net:add(view2)\n",
    "\n",
    "net:type(dtype)  -- Tells Torch to use the CudaTensor type for the network variables, hence using the GPU\n",
    "\n",
    "params, grad_params = net:getParameters()  -- This will return vectors of the randomly initialized network parameters\n",
    "\n",
    "crit = nn.CrossEntropyCriterion():type(dtype)  -- The cross-entropy loss function will be used for optimization\n",
    "\n",
    "print(net)  -- Print network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stacked RNN is now ready to be fed training data and trained using backpropagation through time (BPTT) but we are not going to use this specific implementation in this example.  Instead we will the torch-rnn script [LanguageModel.lua](torch-rnn/LanguageModel.lua) to handle this same initialization of a stacked RNN for NLP for us much more concisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Specify stacked RNN architecture\n",
    "opt.model_type = 'lstm'\n",
    "opt.wordvec_size = 64    -- size of dense real-valued vector representing each character\n",
    "opt.rnn_size = 128       -- number of neurons in each RNN layer\n",
    "opt.num_layers = 2       -- number of stacked RNN layers\n",
    "opt.dropout = 0          -- option to apply a decimal percentage of dropout on feed-forward\n",
    "opt.batchnorm = 0        -- option to use batch normalization\n",
    "opt.cudnn = 1            -- option to use cuDNN native RNN implementation\n",
    "\n",
    "-- Initialize model\n",
    "model = nn.LanguageModel(opt):type(dtype)\n",
    "\n",
    "params, grad_params = model:getParameters()  -- This will return vectors of the randomly initialized network parameters\n",
    "crit = nn.CrossEntropyCriterion():type(dtype)   -- The cross-entropy loss function will be used for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Torch [`optim`](https://github.com/torch/optim) optimization library is used to manage the stochastic gradient descent training process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'optim'\n",
    "\n",
    "N, T = opt.batch_size, opt.seq_length\n",
    "\n",
    "-- Define the loss function that we pass to an optim method\n",
    "-- This function will retrive a batch of data, feed-forward through the model\n",
    "-- compute the error and perform a backpropagation update\n",
    "function f(w)\n",
    "    assert(w == params)\n",
    "    grad_params:zero()\n",
    "    \n",
    "    -- Retrieve next data batch\n",
    "    local x, y = loader:nextBatch('train')\n",
    "    x, y = x:type(dtype), y:type(dtype)\n",
    "    \n",
    "    -- Perform feed-forward operation\n",
    "    local scores = model:forward(x)\n",
    "    \n",
    "    -- Use the Criterion to compute loss; we need to reshape the scores to be\n",
    "    -- two-dimensional before doing so. Annoying.\n",
    "    local scores_view = scores:view(N * T, -1)\n",
    "    local y_view = y:view(N * T)\n",
    "    local loss = crit:forward(scores_view, y_view)\n",
    "    \n",
    "    -- Run the Criterion and model backward to compute gradients\n",
    "    local grad_scores = crit:backward(scores_view, y_view):view(N, T, -1)\n",
    "    model:backward(x, grad_scores)\n",
    "    \n",
    "    grad_params:clamp(-5,5)  -- This is a constraint on the gradient values\n",
    "    \n",
    "    return loss, grad_params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we use a simple loop iteratively calling the loss function `f(w)` defined above.  Executing the cell below should take about 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt.max_epochs = 10\n",
    "\n",
    "local optim_config = {learning_rate = 2e-3}\n",
    "\n",
    "local num_train = loader.split_sizes['train']\n",
    "local num_iterations = opt.max_epochs * num_train\n",
    "\n",
    "model:training()\n",
    "\n",
    "local timer = torch.Timer()\n",
    "\n",
    "for i = 1, num_iterations do\n",
    "    \n",
    "    local epoch = math.floor(i / num_train) + 1\n",
    "    \n",
    "    -- After a complete epoch (pass through the entire training data)\n",
    "    -- reset the model hidden states\n",
    "    if i % num_train == 0 then\n",
    "        model:resetStates()\n",
    "    end\n",
    "    \n",
    "    -- Apply learning rate decay\n",
    "    if epoch % 5 == 0 then\n",
    "        old_lr = optim_config.learning_rate\n",
    "        optim_config = {learning_rate = old_lr * 0.5}\n",
    "    end\n",
    "    \n",
    "    -- Perform optimization update using the ADAM algorithm\n",
    "    local _, loss = optim.adam(f, params, optim_config)\n",
    "    \n",
    "    local float_epoch = i / num_train\n",
    "    \n",
    "    if float_epoch % 1 == 0 then\n",
    "        local msg = 'Epoch %.2f / %d, i = %d / %d, loss = %f'\n",
    "        local args = {msg, float_epoch, opt.max_epochs, i, num_iterations, loss[1]}\n",
    "        print(string.format(unpack(args)))\n",
    "    end\n",
    "end\n",
    "\n",
    "-- Display total training time\n",
    "local msg = '\\n Training took %.2f seconds'\n",
    "local args = {msg, timer:time().real}\n",
    "print(string.format(unpack(args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained we can use it to generate some original random text.  We use the `model:sample()` function to do this.  Note the use of the `temperature` variable.  Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will give the most likely thing that Shakespeare might write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt.length = 2000          -- how many random characters should be generated\n",
    "opt.start_text = ''        -- option to provide some text as a seed for the generation process\n",
    "opt.temperature = 1        -- controls how conservative vs. diverse the sampling is\n",
    "\n",
    "model:evaluate()\n",
    "print(model:sample(opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after just 10 training epochs we see that the RNN has learned to use appropriate vocabulary as well as the typical structure of Shakespeare writing.  It's astonishing to think that the model learned this by seeing only one character at a time - this really demonstrates the sequential memory encoded within an RNN.\n",
    "\n",
    "**Exercise 1:** Feel free to modify the variables in the `opt` dictionary and re-run the cells above.  Things you might want to try are looking at the effect on the quality of generated text by increasing the RNN layer sizes, adding more RNN layers or increasing the sequence length used in the data loader.\n",
    "\n",
    "**Exercise 2:**  What happens to the training time if you change `opt.cudnn` to 0.  This will deactivate the use of the RNN CUDA kernels in cuDNN and instead use the native Torch `cunn` kernels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in training RNNs\n",
    "\n",
    "Unfortunately simple RNNs with many stacked layers can be brittle and difficult to train.  This brittleness arises because the backpropagation of gradients within a neural network is a recursive multiplication process.  This means that if the gradients are small they will shrink exponentially and if they are large they will grow exponentially.  These problems are called \"vanishing\" and \"exploding\" gradients respectively.  One way to look at this unrolling is that it creates a very deep feed-forward neural network, especially for deep RNNs applied to long time sequences.\n",
    "\n",
    "![Backprop in RNNs](files/backpropRNN.png)\n",
    "\n",
    "As you see in the diagram the red arrows representing the backpropagation of gradients accumulate in very long chains for an RNN on the right versus the feed-forward network on the left and so can easily vanish or explode through multiplication.  For a much more detailed explanation of these problems, see [this](https://www.youtube.com/watch?v=Pp4oKq4kCYs) video by Geoffrey Hinton.\n",
    "\n",
    "One of the most popular solutions for preventing vanishing and exploding gradients is a modified network structure called Long Short Term Memory (LSTM).  For a thorough understanding of LSTM networks, after completing this lab, I recommend you read [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs) blog post.  LSTMs were introduced by [Hochreiter & Schmidhuber](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) in 1997 and have been subsequently refined and put to good use by many people.\n",
    "\n",
    "For now the key idea to remember about LSTM layers is that they add gating functions that can control how much the next network state is determined by new input and how much it is determined by what has already been seen. By allowing the network to learn how to control these gates, vanishing and exploding gradients can be avoided.\n",
    "\n",
    "LSTM layers can be viewed as a drop-in replacement for the vanilla RNN layer we have already defined.  In `torch-rnn` we can choose to make this replacement by setting the variable `opt.model_type` to 'lstm'.  \n",
    "\n",
    "**Exercise 3:**  Modify in the code above to set `opt.model_type = 'lstm'` and re-run the training.  What happens to model accuracy and training speed?  \n",
    "\n",
    "**Exercise 4:**  Set `opt.batch_norm = 1` to apply batch normalization and `opt.dropout = 0.2` to apply dropout regularization.  What happens to the quality of generated text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Answers to exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** You will find that a longer training sequence length, more RNN layers or a larger RNN will all improve the quality of the generated text but will also increase training time for the network.\n",
    "\n",
    "**Answer 2:** You will find that training time is about 1.5X slower.  This is because cuDNN v5 introduced a number of optimizations for RNN layers, for example fusing of multiple stacked layers into a smaller number of more efficient operations.  You can read more about these optimizations in [this blog post](https://devblogs.nvidia.com/parallelforall/optimizing-recurrent-neural-networks-cudnn-5/).\n",
    "\n",
    "**Answer 3:** You will find that training time takes a little longer due to the added complexity of an LSTM layer versus an RNN layer.  You may also assess that the quality of the generated text is worse. This is due to the LSTM overfitting the training data. \n",
    "\n",
    "**Answer 4:** With these options you should see a much higher quality output from the LSTM model versus the original RNN model.  These regularization techniques prevent overfitting in the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
